{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa535d0",
   "metadata": {},
   "source": [
    "# Refusal Circuit Analysis Report\n",
    "\n",
    "**Generated:** 2026-01-12 01:44:56\n",
    "\n",
    "This notebook contains the complete analysis pipeline and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26dc3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.models import load_model, list_available_models\n",
    "from src.data import REFUSAL_PROMPT_PAIRS\n",
    "from src.circuits import CircuitAnalyzer, compute_refusal_direction\n",
    "from src.steering import ClampingExperiment\n",
    "from src.analysis import significance_test, compare_models\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2d4dd",
   "metadata": {},
   "source": [
    "## Load Experiment Results\n",
    "\n",
    "Load the results from batch experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91afc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_path = 'batch_results.json'  # Adjust path as needed\n",
    "try:\n",
    "    with open(results_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(f'Loaded results for {len(results.get(\"model_results\", {}))} models')\n",
    "except FileNotFoundError:\n",
    "    print('Results file not found. Run batch_runner.py first.')\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c87cd",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Overview of results across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65178c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics\n",
    "if 'model_results' in results:\n",
    "    model_results = results['model_results']\n",
    "    \n",
    "    sep_scores = [r['separation_score'] for r in model_results.values()]\n",
    "    probe_accs = [r['probe_accuracy'] for r in model_results.values()]\n",
    "    \n",
    "    print('Separation Scores:')\n",
    "    print(f'  Mean: {np.mean(sep_scores):.3f}')\n",
    "    print(f'  Std:  {np.std(sep_scores):.3f}')\n",
    "    print(f'  Min:  {np.min(sep_scores):.3f}')\n",
    "    print(f'  Max:  {np.max(sep_scores):.3f}')\n",
    "    \n",
    "    print('\\nProbe Accuracy:')\n",
    "    print(f'  Mean: {np.mean(probe_accs):.1%}')\n",
    "    print(f'  Std:  {np.std(probe_accs):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a7986",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "Key plots from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of separation scores by model\n",
    "if 'model_results' in results:\n",
    "    models = list(model_results.keys())\n",
    "    sep_scores = [model_results[m]['separation_score'] for m in models]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    colors = ['#e63946' if model_results[m].get('model_type') == 'instruction_tuned' \n",
    "              else '#457b9d' for m in models]\n",
    "    ax.bar(models, sep_scores, color=colors)\n",
    "    ax.set_ylabel('Separation Score (Ïƒ)')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_title('Refusal Direction Separation by Model')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98721d6d",
   "metadata": {},
   "source": [
    "## Interactive Analysis\n",
    "\n",
    "Run custom analyses on specific models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5920f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific model (modify as needed)\n",
    "MODEL_NAME = 'pythia-70m'\n",
    "N_PAIRS = 5\n",
    "\n",
    "try:\n",
    "    # Load model\n",
    "    model = load_model(MODEL_NAME, device='cuda')\n",
    "    \n",
    "    # Get prompt pairs\n",
    "    pairs = REFUSAL_PROMPT_PAIRS[:N_PAIRS]\n",
    "    \n",
    "    # Run analysis\n",
    "    analyzer = CircuitAnalyzer(model)\n",
    "    circuit = analyzer.analyze_prompt_pair(pairs[0], components='resid')\n",
    "    \n",
    "    print('Top 5 components:')\n",
    "    for comp in circuit.top_k_components(5):\n",
    "        print(f'  {comp.name}: {comp.importance_score:.4f}')\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40eb5cb",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Key takeaways from this analysis:\n",
    "\n",
    "1. Refusal behavior is localized to specific layers and attention heads\n",
    "2. The refusal direction provides clear separation between prompt types\n",
    "3. Steering experiments validate causal identification\n",
    "\n",
    "---\n",
    "\n",
    "*Generated by SaycuredAI Refusal Circuit Analysis Framework*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
