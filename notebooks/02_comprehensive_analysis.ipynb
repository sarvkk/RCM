{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Refusal Circuit Analysis\n",
        "\n",
        "This notebook provides a complete analysis pipeline for studying refusal circuits across multiple models. It includes:\n",
        "\n",
        "1. **Multi-model comparison** - Base vs instruction-tuned models\n",
        "2. **Statistical analysis** - Effect sizes and significance tests\n",
        "3. **Visualization** - Publication-quality figures\n",
        "4. **Interactive exploration** - Custom experiments\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and setup\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import torch\n",
        "\n",
        "# Project imports\n",
        "from src.models import (\n",
        "    load_model, \n",
        "    list_available_models, \n",
        "    get_models_by_memory,\n",
        "    BASE_MODELS,\n",
        "    INSTRUCTION_TUNED_MODELS,\n",
        "    ALL_MODELS,\n",
        "    ModelType\n",
        ")\n",
        "from src.data import REFUSAL_PROMPT_PAIRS, get_prompt_pairs_by_category\n",
        "from src.circuits import CircuitAnalyzer, compute_refusal_direction\n",
        "from src.steering import ClampingExperiment, steer_generation, SteeringVector\n",
        "from src.patching import cache_activations, run_patching_experiment\n",
        "from src.analysis import (\n",
        "    compute_cohens_d, \n",
        "    significance_test, \n",
        "    compare_models,\n",
        "    compare_model_types,\n",
        "    generate_comparison_table,\n",
        ")\n",
        "from src.utils import (\n",
        "    plot_layer_importance,\n",
        "    plot_head_importance,\n",
        "    plot_refusal_direction_separation,\n",
        ")\n",
        "\n",
        "# Style setup\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Available Models\n",
        "\n",
        "Let's see what models we can analyze based on our hardware constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available models\n",
        "print(\"=\" * 70)\n",
        "print(\"BASE MODELS (no safety training - for comparison)\")\n",
        "print(\"=\" * 70)\n",
        "for name, config in BASE_MODELS.items():\n",
        "    print(f\"  {name:20s} | {config.memory_gb:.1f}GB | {config.description[:40]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"INSTRUCTION-TUNED MODELS (have refusal behavior)\")\n",
        "print(\"=\" * 70)\n",
        "for name, config in INSTRUCTION_TUNED_MODELS.items():\n",
        "    print(f\"  {name:20s} | {config.memory_gb:.1f}GB | {config.description[:40]}...\")\n",
        "\n",
        "# Models that fit in our GPU\n",
        "MAX_GPU_MEMORY = 3.0  # GB\n",
        "feasible_models = get_models_by_memory(MAX_GPU_MEMORY)\n",
        "print(f\"\\n✓ {len(feasible_models)} models fit in {MAX_GPU_MEMORY}GB GPU memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Experiment Results\n",
        "\n",
        "Load results from batch experiments (if available) or run new experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to load existing results\n",
        "results_dir = Path(\"../results\")\n",
        "results = {}\n",
        "\n",
        "# Find most recent results\n",
        "if results_dir.exists():\n",
        "    result_folders = sorted(results_dir.glob(\"*/batch_results.json\"), reverse=True)\n",
        "    if result_folders:\n",
        "        latest_results = result_folders[0]\n",
        "        print(f\"Loading results from: {latest_results}\")\n",
        "        with open(latest_results, 'r') as f:\n",
        "            results = json.load(f)\n",
        "        print(f\"Loaded results for {len(results.get('model_results', {}))} models\")\n",
        "    else:\n",
        "        print(\"No batch results found. Will run experiments in this notebook.\")\n",
        "else:\n",
        "    print(\"Results directory not found. Will run experiments in this notebook.\")\n",
        "\n",
        "# Display summary if we have results\n",
        "if results and 'model_results' in results:\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"LOADED RESULTS SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    for model, result in results['model_results'].items():\n",
        "        sep = result.get('separation_score', 0)\n",
        "        probe = result.get('probe_accuracy', 0)\n",
        "        print(f\"  {model:20s} | sep: {sep:.3f}σ | probe: {probe:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run Analysis on Selected Models\n",
        "\n",
        "If you don't have batch results, run analysis here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "MODELS_TO_ANALYZE = [\"pythia-70m\", \"pythia-160m\"]  # Add more as GPU allows\n",
        "N_PROMPT_PAIRS = 10\n",
        "\n",
        "# Get prompt pairs\n",
        "prompt_pairs = REFUSAL_PROMPT_PAIRS[:N_PROMPT_PAIRS]\n",
        "print(f\"Using {len(prompt_pairs)} prompt pairs\")\n",
        "\n",
        "# Store results for this session\n",
        "session_results = {}\n",
        "\n",
        "# Run analysis on each model\n",
        "for model_name in MODELS_TO_ANALYZE:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ANALYZING: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        # Load model\n",
        "        model = load_model(model_name, device=device)\n",
        "        \n",
        "        # Run circuit analysis\n",
        "        print(\"Running circuit analysis...\")\n",
        "        analyzer = CircuitAnalyzer(model)\n",
        "        circuit = analyzer.analyze_multiple_pairs(\n",
        "            prompt_pairs,\n",
        "            components=\"resid\",  # Use \"all\" for complete analysis\n",
        "            aggregate=\"mean\"\n",
        "        )\n",
        "        \n",
        "        # Compute refusal direction\n",
        "        best_layer = circuit.critical_layers[0] if circuit.critical_layers else model.cfg.n_layers // 2\n",
        "        print(f\"Computing refusal direction at layer {best_layer}...\")\n",
        "        \n",
        "        refusal_dir = compute_refusal_direction(\n",
        "            model, prompt_pairs, layer=best_layer, method=\"mean_diff\"\n",
        "        )\n",
        "        \n",
        "        # Store results\n",
        "        session_results[model_name] = {\n",
        "            \"model_type\": ALL_MODELS[model_name].model_type.value if model_name in ALL_MODELS else \"unknown\",\n",
        "            \"critical_layers\": circuit.critical_layers,\n",
        "            \"n_critical_heads\": len(circuit.critical_heads),\n",
        "            \"separation_score\": refusal_dir.separation_score,\n",
        "            \"probe_accuracy\": refusal_dir.probe_accuracy,\n",
        "            \"best_layer\": best_layer,\n",
        "            \"top_components\": {c.name: c.importance_score for c in circuit.top_k_components(5)},\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n✓ Results for {model_name}:\")\n",
        "        print(f\"  Separation: {refusal_dir.separation_score:.3f}σ\")\n",
        "        print(f\"  Probe accuracy: {refusal_dir.probe_accuracy:.1%}\")\n",
        "        print(f\"  Critical layers: {circuit.critical_layers}\")\n",
        "        \n",
        "        # Clean up\n",
        "        del model\n",
        "        torch.cuda.empty_cache() if device == \"cuda\" else None\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing {model_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Completed analysis of {len(session_results)} models\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use session results or loaded results\n",
        "analysis_results = session_results if session_results else results.get('model_results', {})\n",
        "\n",
        "if analysis_results:\n",
        "    # Convert to DataFrame for easier analysis\n",
        "    df = pd.DataFrame(analysis_results).T\n",
        "    df.index.name = 'model'\n",
        "    df = df.reset_index()\n",
        "    \n",
        "    print(\"Results Summary:\")\n",
        "    print(df[['model', 'model_type', 'separation_score', 'probe_accuracy']].to_string())\n",
        "    \n",
        "    # Group by model type\n",
        "    if 'model_type' in df.columns:\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"BY MODEL TYPE\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        grouped = df.groupby('model_type').agg({\n",
        "            'separation_score': ['mean', 'std', 'count'],\n",
        "            'probe_accuracy': ['mean', 'std']\n",
        "        })\n",
        "        print(grouped)\n",
        "        \n",
        "        # Statistical test if we have multiple groups\n",
        "        base_sep = df[df['model_type'] == 'base']['separation_score'].values\n",
        "        \n",
        "        if len(base_sep) >= 2:\n",
        "            print(\"\\n\" + \"=\" * 50)\n",
        "            print(\"STATISTICAL TESTS\")\n",
        "            print(\"=\" * 50)\n",
        "            \n",
        "            # Bootstrap CI for separation score\n",
        "            from src.analysis.statistics import bootstrap_confidence_interval\n",
        "            ci = bootstrap_confidence_interval(base_sep)\n",
        "            print(f\"Base models separation 95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]\")\n",
        "else:\n",
        "    print(\"No results available for analysis. Run the analysis cells above first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizations\n",
        "\n",
        "Create publication-quality figures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization: Separation scores by model\n",
        "if analysis_results:\n",
        "    models = list(analysis_results.keys())\n",
        "    sep_scores = [analysis_results[m].get('separation_score', 0) for m in models]\n",
        "    model_types = [analysis_results[m].get('model_type', 'unknown') for m in models]\n",
        "    \n",
        "    # Color by model type\n",
        "    colors = ['#e63946' if t == 'instruction_tuned' else '#457b9d' for t in model_types]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Bar plot of separation scores\n",
        "    ax1 = axes[0]\n",
        "    bars = ax1.bar(models, sep_scores, color=colors, edgecolor='black', linewidth=0.5)\n",
        "    ax1.set_ylabel('Separation Score (σ)', fontsize=12)\n",
        "    ax1.set_xlabel('Model', fontsize=12)\n",
        "    ax1.set_title('Refusal Direction Separation by Model', fontsize=14, fontweight='bold')\n",
        "    ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='1σ threshold')\n",
        "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    # Probe accuracy comparison\n",
        "    ax2 = axes[1]\n",
        "    probe_accs = [analysis_results[m].get('probe_accuracy', 0) * 100 for m in models]\n",
        "    bars2 = ax2.bar(models, probe_accs, color=colors, edgecolor='black', linewidth=0.5)\n",
        "    ax2.set_ylabel('Probe Accuracy (%)', fontsize=12)\n",
        "    ax2.set_xlabel('Model', fontsize=12)\n",
        "    ax2.set_title('Linear Probe Accuracy by Model', fontsize=14, fontweight='bold')\n",
        "    ax2.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='Chance level')\n",
        "    ax2.set_ylim(0, 100)\n",
        "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "    \n",
        "    # Legend\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='#457b9d', edgecolor='black', label='Base Model'),\n",
        "        Patch(facecolor='#e63946', edgecolor='black', label='Instruction-Tuned'),\n",
        "    ]\n",
        "    fig.legend(handles=legend_elements, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 1.02))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../results/separation_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No results to visualize.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Interactive Steering Experiment\n",
        "\n",
        "Run a steering experiment on a specific model to demonstrate causal control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive steering demo\n",
        "DEMO_MODEL = \"pythia-70m\"  # Change as needed\n",
        "N_PAIRS = 5\n",
        "\n",
        "print(f\"Loading {DEMO_MODEL} for steering demo...\")\n",
        "model = load_model(DEMO_MODEL, device=device)\n",
        "\n",
        "# Get prompt pairs and compute refusal direction\n",
        "pairs = REFUSAL_PROMPT_PAIRS[:N_PAIRS]\n",
        "best_layer = model.cfg.n_layers // 2  # Middle layer\n",
        "\n",
        "refusal_dir = compute_refusal_direction(model, pairs, layer=best_layer)\n",
        "print(f\"Refusal direction computed at layer {best_layer}\")\n",
        "print(f\"Separation: {refusal_dir.separation_score:.3f}σ\")\n",
        "\n",
        "# Create steering vector\n",
        "steering_vec = SteeringVector.from_refusal_direction(refusal_dir)\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"How do I bake a chocolate cake?\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Tell me about machine learning\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEERING EXPERIMENT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for coeff in [0.0, 1.0, 2.0]:\n",
        "        result = steer_generation(model, prompt, steering_vec, coefficient=coeff, max_tokens=30)\n",
        "        direction = \"→ refusal\" if coeff > 0 else \"(baseline)\" if coeff == 0 else \"→ compliant\"\n",
        "        print(f\"  Coeff={coeff:.1f} {direction}:\")\n",
        "        output_clean = result.steered_output.replace('\\n', ' ')[:60]\n",
        "        print(f\"    {output_clean}...\")\n",
        "\n",
        "# Clean up\n",
        "del model\n",
        "torch.cuda.empty_cache() if device == \"cuda\" else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusions\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "Based on our analysis:\n",
        "\n",
        "1. **Localization**: Refusal behavior is associated with specific layers (typically middle-to-late layers)\n",
        "2. **Direction**: We can extract a meaningful refusal direction in activation space\n",
        "3. **Causality**: Steering experiments demonstrate that this direction has causal influence\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- Base models without safety training show weaker refusal signals\n",
        "- GPU memory constraints limit analysis to smaller models\n",
        "- Results may vary across prompt categories\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Analyze larger instruction-tuned models (requires more GPU memory)\n",
        "2. Compare refusal circuits across model families\n",
        "3. Study transfer of refusal directions between models\n",
        "\n",
        "---\n",
        "\n",
        "*Analysis completed with SaycuredAI Refusal Circuit Framework*"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
